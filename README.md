> The limits of my language mean the limits of my world. - Ludwig Wittgenstein

This repository serves as a central hub for all things related to Large Language Models (LLMs). Whether you‚Äôre a researcher, developer, or simply curious about LLMs, you‚Äôll find valuable content here to enhance your understanding and work with these remarkable language models. 

---
## üìå Template
- YYMMDD - Term - Title - Citation - [link](https://www.arsenal.com)
---
## üìú Survey
- 230331 - Survey - A Survey of Large Language Models - 2070 - [arXiv](https://arxiv.org/abs/2303.18223)
---
## üëë Transformer
- 170612 - Transformer - Attention is All You Need - 129026 - [arXiv](https://arxiv.org/abs/1706.03762)
- 181011 - BERT - Pre-Training of Deep Bidirectional Transformers for Language Understanding - 108937 - [arXiv](https://arxiv.org/abs/1810.04805)
- 191023 - T5 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 17213 - [arXiv](https://arxiv.org/abs/1910.10683)
- 191029 - BART - Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension - 10266 - [arXiv](https://arxiv.org/abs/1910.13461)
- 210226 - CLIP - Learning Transferable Visual Models From Natural Language Supervision - 19893 - [arXiv](https://arxiv.org/abs/2103.00020)
---
## üì∏ Transformer in Vision
- 200526 - DETR - End-to-End Object Detection with Transformers - 12801 - [arXiv](https://arxiv.org/abs/2005.12872)
- 201022 - ViT - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale - 41454 - [arXiv](https://arxiv.org/abs/2010.11929)
- 210325 - Swin Transformer - Hierarchical Vision Transformer using Shifted Windows - 20459 - [arXiv](https://arxiv.org/abs/2103.14030)
---
## üö© Feature
- 200123 - Scaling Law - Scaling Laws for Neural Language Models - 1972 - [arXiv](https://arxiv.org/abs/2001.08361)
- 220615 - Emergent Ability - Emergent Abilities of Large Language Models - 2028 - [arXiv](https://arxiv.org/abs/2206.07682)
---
## üôÇ GPT
- 180611 - GPT - Improving Language Understanding by Generative Pre-Training - 10329 - [report](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- 190214 - GPT-2 - Language Models are Unsupervised Multitask Learners - 12538 - [report](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- 200528 - GPT-3 - Language Models are Few-Shot Learners - 29926 - [arXiv](https://arxiv.org/abs/2005.14165)
- 220304 - InstructGPT - Training language models to follow instructions with human feedback - 8115 - [arXiv](https://arxiv.org/abs/2203.02155)
- 230315 - GPT-4 - Technical Report - 2858 - [arXiv](https://arxiv.org/abs/2303.08774)
- 230925 - GPT-4V(ision) System Card - [report](https://cdn.openai.com/papers/GPTV_System_Card.pdf)
- 240808 - GPT-4o System Card - [report](https://cdn.openai.com/gpt-4o-system-card.pdf)
- 240912 - OpenAI o1 System Card - [report](https://assets.ctfassets.net/kftzwdyauwt9/67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf)
---
## üòä Llama
- 230227 - Llama - Open and Efficient Foundation Language Models - 7880 - [arXiv](https://arxiv.org/abs/2302.13971)
- 230718 - Llama 2 - Open Foundation and Fine-Tuned Chat Models - 7047 - [arXiv](https://arxiv.org/abs/2307.09288)
- 240723 - Llama 3.1 - The Llama 3 Herd of Models - 5 - [report](https://scontent.fsin15-2.fna.fbcdn.net/v/t39.2365-6/452387774_1036916434819166_4173978747091533306_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=t6egZJ8QdI4Q7kNvgFtFzvs&_nc_ht=scontent.fsin15-2.fna&oh=00_AYB-GFK6fEfINdSp2aOXtidS6BdWj-eRNVdnni0UD70p3Q&oe=66A67B0D)
---
## üòã Gemini
- 231219 - Gemini - A Family of Highly Capable Multimodal Models - 1170 - [arXiv](https://arxiv.org/abs/2312.11805)
- 240308 - Gemini 1.5 - Unlocking Multimodal Understanding across Millions of Tokens of Context - 232 - [arXiv](https://arxiv.org/abs/2403.05530)
---
## ü§≠ Gemma
- 240221 - Gemma - Open Models Based on Gemini Research and Technology - 302 - [report](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)
- 240627 - Gemma 2 - Improving Open Language Models at a Practical Size - 2 - [report](https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf)
---
## üòè Mistral
- 231010 - Mistral 7B - Mistral 7B - 497 - [arXiv](https://arxiv.org/abs/2310.06825)
- 240108 - Mixtral 8x7B - Mixtral of Experts - 611 - [arXiv](https://arxiv.org/abs/2401.04088)
---
## üòé LLaVA
- 230417 - LLaVA - Visual Instruction Tuning - 2663 - [arXiv](https://arxiv.org/abs/2304.08485)
- 231005 - LLaVA-1.5 - Improved Baselines with Visual Instruction Tuning - 810 - [arXiv](https://arxiv.org/abs/2310.03744)
- 240710 - LLaVA-NeXT-Interleave - Tackling Multi-image, Video, and 3D in Large Multimodal Models - 2 - [arXiv](https://arxiv.org/abs/2407.07895)
- 240806 - LLaVA-OneVision - Easy Visual Task Transfer - 0 - [arXiv](https://arxiv.org/abs/2408.03326)
---
## üöÄ Reinforcement Learning from Human Feedback
- 170612 - Reinforcement Learning - Deep Reinforcement Learning from Human Preferences - 2587 - [arXiv](https://arxiv.org/abs/1706.03741)
- 170720 - PPO - Proximal Policy Optimization Algorithms - 19382 - [arXiv](https://arxiv.org/abs/1707.06347)
- 200902 - Human Feedback - Learning to Summarize from Human Feedback - 1345 - [arXiv](https://arxiv.org/abs/2009.01325)
- 230529 - DPO - Direct Preference Optimization: Your Language Model is Secretly a Reward Model - 1104 - [arXiv](https://arxiv.org/abs/2305.18290)
---
## üë®‚Äçüîß Fine-Tuning
- 190202 - Adapter Tuning - Parameter-Efficient Transfer Learning for NLP - 3340 - [arXiv](https://arxiv.org/abs/1902.00751)
- 210101 - Prefix Tuning - Optimizing Continuous Prompts for Generation - 3190 - [arXiv](https://arxiv.org/abs/2101.00190)
- 210418 - Prompt Tuning - The Power of Scale for Parameter-Efficient Prompt Tuning - 2948 - [arXiv](https://arxiv.org/abs/2104.08691)
- 210617 - LoRA - Low-Rank Adaptation of Large Language Models - 5793 - [arXiv](https://arxiv.org/abs/2106.09685)
- 230523 - QLoRA - Efficient Finetuning of Quantized LLMs - 1221 - [arXiv](https://arxiv.org/abs/2305.14314)
---
## üëç Prompting
- 220128 - Chain-of-Thought - Prompting Elicits Reasoning in Large Language Models - 6455 - [arXiv](https://arxiv.org/abs/2201.11903)
- 221006 - ReAct - Synergizing Reasoning and Acting in Language Models - 1291 - [arXiv](https://arxiv.org/abs/2210.03629)
---
## üìö Retrieval Augmented Generation
- 200522 - RAG - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks - 3104 - [arXiv](https://arxiv.org/abs/2005.11401)
- 240315 - RAFT - Adapting Language Model to Domain Specifc RAG - 50 - [arXiv](https://arxiv.org/abs/2403.10131)
- 240424 - Graph RAG - From Local to Global A Graph RAG Approach to Query-Focused Summarization - 10 - [arXiv](https://arxiv.org/abs/2404.16130)
---
## üß∞ Tool
- 230209 - Toolformer - Language Models Can Teach Themselves to Use Tools - 989 - [arXiv](https://arxiv.org/abs/2302.04761)
---
## üß† Artificial General Intelligence
- 230322 - AGI - Sparks of Artificial General Intelligence: Early experiments with GPT-4 - 2604 - [arXiv](https://arxiv.org/abs/2303.12712)
## üíÇ Safety
- 231207 - CyberSecEval - A Secure Coding Benchmark for Language Models - 29 - [arXiv](https://arxiv.org/abs/2312.04724)
- 231207 - Llama Guard - LLM-based Input-Output Safeguard for Human-AI Conversations - 121 - [arXiv](https://arxiv.org/abs/2312.06674)
- 240419 - CyberSecEval 2 - A Wide-Ranging Cybersecurity Evaluation Suite for Large Language Models - 8 - [arXiv](https://arxiv.org/abs/2404.13161)
- 240731 - ShieldGemma - Generative AI Content Moderation Based on Gemma - 0 - [arXiv](https://arxiv.org/abs/2407.21772)
- 240802 - CyberSecEval 3 - Advancing the Evaluation of Cybersecurity Risks and Capabilities in Large Language Models - 0 - [arXiv](https://arxiv.org/abs/2408.01605)
---
## üí° Contributing

This project holds great significance for me, and I‚Äôm constantly seeking ways to improve it. If you have ideas, fixes, or even just suggestions, please share them! I‚Äôm open to discussing any pull requests, especially if we need to align them with the LLM‚Äôs goals.

Thansk a lot!

---
## ‚≠ê Star History
[![Star History Chart](https://api.star-history.com/svg?repos=Jingkou1012/LLM-Paper&type=Date)](https://star-history.com/#Jingkou1012/LLM-Paper&Date)
