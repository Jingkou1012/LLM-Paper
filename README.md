> The limits of my language mean the limites of my world. - Ludwig Wittgenstein

This repository serves as a central hub for all things related to Large Language Models (LLMs). Whether you‚Äôre a researcher, developer, or simply curious about LLMs, you‚Äôll find valuable content here to enhance your understanding and work with these remarkable language models. 

---
## üìå Template
- YYMMDD - Term - Title - Citation - [link](https:)
---
## üìú Survey
- 230331 - Survey - A Survey of Large Language Models - 2070 - [arXiv](https://arxiv.org/abs/2303.18223)
---
## üìù Natural Langugae Processing
- 130116 - Word2Vec - Efficient Estimation of Word Representations in Vector Space - 43194 - [arXiv](https://arxiv.org/abs/1301.3781)
## üëë Transformer
- 170612 - Transformer - Attention is All You Need - 129026 - [arXiv](https://arxiv.org/abs/1706.03762)
- 181011 - BERT - Pre-Training of Deep Bidirectional Transformers for Language Understanding - 108937 - [arXiv](https://arxiv.org/abs/1810.04805)
- 191023 - T5 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - 17213 - [arXiv](https://arxiv.org/abs/1910.10683)
- 191029 - BART - Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension - 10266 - [arXiv](https://arxiv.org/abs/1910.13461)
---
## üôÇ GPT
- 180611 - GPT - Improving Language Understanding by Generative Pre-Training - 10329 - [report](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- 190214 - GPT-2 - Language Models are Unsupervised Multitask Learners - 12538 - [report](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- 200528 - GPT-3 - Language Models are Few-Shot Learners - 29926 - [arXiv](https://arxiv.org/abs/2005.14165)
- 220304 - InstructGPT - Training language models to follow instructions with human feedback - 8115 - [arXiv](https://arxiv.org/abs/2203.02155)
- 230315 - GPT-4 - Technical Report - 2858 - [arXiv](https://arxiv.org/abs/2303.08774)
---
## üòä Llama
- 230227 - Llama - Open and Efficient Foundation Language Models - 7880 - [arXiv](https://arxiv.org/abs/2302.13971)
- 230718 - Llama 2 - Open Foundation and Fine-Tuned Chat Models - 7047 - [arXiv](https://arxiv.org/abs/2307.09288)
- 240723 - Llama 3.1 - The Llama 3 Herd of Models - 5 [report](https://scontent.fsin15-2.fna.fbcdn.net/v/t39.2365-6/452387774_1036916434819166_4173978747091533306_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=t6egZJ8QdI4Q7kNvgFtFzvs&_nc_ht=scontent.fsin15-2.fna&oh=00_AYB-GFK6fEfINdSp2aOXtidS6BdWj-eRNVdnni0UD70p3Q&oe=66A67B0D)
---
## üòã Gemini
- 231219 - Gemini - A Family of Highly Capable Multimodal Models - 1170 - [arXiv](https://arxiv.org/abs/2312.11805)
- 240308 - Gemini 1.5 - Unlocking Multimodal Understanding across Millions of Tokens of Context - 232 - [arXiv](https://arxiv.org/abs/2403.05530)
---
## ü§≠ Gemma
- 240221 - Gemma - Open Models Based on Gemini Research and Technology - 302 - [report](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)
- 240627 - Gemma 2 - Improving Open Language Models at a Practical Size - 2 - [report](https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf)
---
## üöÄ Reinforcement Learning from Human Feedback
- 170612 - Reinforcement Learning - Deep Reinforcement Learning from Human Preferences - 2587 - [arXiv](https://arxiv.org/abs/1706.03741)
- 170720 - PPO - Proximal Policy Optimization Algorithms - 19382 - [arXiv](https://arxiv.org/abs/1707.06347)
- 200902 - Human Feedback - Learning to Summarize from Human Feedback - 1345 - [arXiv](https://arxiv.org/abs/2009.01325)
- 230529 - DPO - Direct Preference Optimization: Your Language Model is Secretly a Reward Model - 1104 - [arXiv](https://arxiv.org/abs/2305.18290)
---
## üë®‚Äçüîß Fine-Tuning
- 190202 - Adapter Tuning - Parameter-Efficient Transfer Learning for NLP - 3340 - [arXiv](https://arxiv.org/abs/1902.00751)
- 210101 - Prefix Tuning - Optimizing Continuous Prompts for Generation - 3190 - [arXiv](https://arxiv.org/abs/2101.00190)
- 210418 - Prompt Tuning - The Power of Scale for Parameter-Efficient Prompt Tuning - 2948 - [arXiv](https://arxiv.org/abs/2104.08691)
- 210617 - LoRA - Low-Rank Adaptation of Large Language Models - 5793 - [arXiv](https://arxiv.org/abs/2106.09685)
- 230523 - QLoRA - Efficient Finetuning of Quantized LLMs - 1221 - [arXiv](https://arxiv.org/abs/2305.14314)
---
## üëç Prompting
- 220128 - Chain-of-Thought - Prompting Elicits Reasoning in Large Language Models - 6455 - [arXiv](https://arxiv.org/abs/2201.11903)
---
## üìö Retrieval Augmented Generation
- 200522 - RAG - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks - 3104 - [arXiv](https://arxiv.org/abs/2005.11401)
- 240424 - Graph RAG - From Local to Global A Graph RAG Approach to Query-Focused Summarization - 10 - [arXiv](https://arxiv.org/abs/2404.16130)
---
## üî• Feature
- 160721 - Layer Normalization - Layer Normalization - 12166 - [arXiv](https://arxiv.org/abs/1607.06450)
---
## üß∞ Tool
- 230209 - Toolformer - Language Models Can Teach Themselves to Use Tools - 989 - [arXiv](https://arxiv.org/abs/2302.04761)
---
## üì∏ Transformer in Vision
- 200526 - DETR - End-to-End Object Detection with Transformers - 12801 - [arXiv](https://arxiv.org/abs/2005.12872)
- 201022 - ViT - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale - 41454 - [arXiv](https://arxiv.org/abs/2010.11929)
- 210325 - Swin Transformer - Hierarchical Vision Transformer using Shifted Windows - 20459 - [arXiv](https://arxiv.org/abs/2103.14030)
---
## üíû Multimodal
- 210226 - CLIP - Learning Transferable Visual Models From Natural Language Supervision - 19893 - [arXiv](https://arxiv.org/abs/2103.00020)
---
## üß† Artificial General Intelligence
- 230322 - AGI - Sparks of Artificial General Intelligence: Early experiments with GPT-4 - 2604 - [arXiv](https://arxiv.org/abs/2303.12712)
## üí° Contributing

This project holds great significance for me, and I‚Äôm constantly seeking ways to improve it. If you have ideas, fixes, or even just suggestions, please share them! I‚Äôm open to discussing any pull requests, especially if we need to align them with the LLM‚Äôs goals.

Thansk a lot!

---
## ‚≠ê Star History
[![Star History Chart](https://api.star-history.com/svg?repos=Jingkou1012/LLM-Paper&type=Date)](https://star-history.com/#Jingkou1012/LLM-Paper&Date)
